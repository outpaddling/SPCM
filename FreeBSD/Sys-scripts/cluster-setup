#!/bin/sh -e

##########################################################################
#   Description:
#       Automate setup of a simple FreeBSD cluster.
#       Installs common software and configures settings to facilitate
#       cluster operation.  Assumes a single head node will handle
#       job initiation/scheduling and act as a file server for all nodes.
#
#   Usage:
#       First, run 
#
#           cluster-setup head
#
#       on the head node.
#
#   Then, run
#
#           cluster-setup compute
#
#       on the rest.
#       
#   History:
#       Dec 2009    J Bacon
##########################################################################


usage()
{
    printf "Usage: $0 head|compute|io|vis\n"
    exit 1
}


line()
{
    printf "==============================================================================\n"
}


pause()
{
    local junk
    printf "Press enter to continue..."
    read junk
}


##########################################################################
#   Function description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2013-12-03  Charlie &   Begin
##########################################################################

stop_service()
{
    if [ $# != 1 ]; then
	printf "Usage: stop_service service\n"
    fi
    service $1 stop > /dev/null 2>&1 || true
    sleep 2
    killall $1 > /dev/null 2>&1 || true
    sleep 1
    killall -9 $1 > /dev/null 2>&1 || true
}


##########################################################################
#   Description:
#       Install ports or packages specified by arguments if they are not
#       already installed.  If $AUTO_BUILD_FROM_SOURCE is true, installs from source,
#       otherwise attempts to install using pkg add.
#
#   Arguments:
#       List of ports in the for "category/name"
##########################################################################

install_packages()
{
    printf "Installing:\n"
    for pkg in $*; do
    {
	printf "\t$pkg...\n"
	if [ ! -e $LOG_DIR ]; then
	    mkdir $LOG_DIR
	fi
	log_file=$LOG_DIR/`echo $pkg | tr / -`
	if ! auto-install-packages -l $pkg 2>&1 | tee $log_file; then
	    more $log_file
	    exit 1
	fi
    }
    done
}


add_node_name()
{
    if [ $# != 2 ]; then
	printf "Usage: $0 node-name node-file\n"
	exit 1
    fi
    
    name=$1
    file=$2
    
    # Prevent fgrep from failing
    if [ ! -e $file ]; then
	touch $file
    fi
    
    if ! fgrep -q $name $file; then
	printf "$name\n" >> $file
    fi
}


install_scheduler()
{
    line
    install_packages sysutils/slurm-wlm
    pw usermod slurm -d /home/slurm -m
}


install_mpi()
{
    line
    # Install OpenMPI from source so we can choose build options
    if ! auto-package-installed net/openmpi; then
	printf "\nInstalling openmpi...\n"
	install_packages net/openmpi
    fi
}


slurm_config()
{
    # FIXME: creating these directories should not be necessary
    # --syslog?
    mkdir -p /var/log/munge /var/run/munge /var/lib/munge
    mkdir -p /var/log/slurm /var/run/slurm
    chown slurm:slurm /var/log/slurm /var/run/slurm
    
    # Generate munge.key
    service munged stop > /dev/null 2>&1 || true
    munge_dir="$LOCALBASE/etc/munge"
    if [ $NODE_TYPE = 'head' ]; then
	if [ ! -e $munge_dir/munge.key ]; then
	    mkdir -p -m 0700 $munge_dir
	    touch $munge_dir/munge.key
	    chmod 400 $munge_dir/munge.key
	    printf "Generating munge key...\n"
	    dd if=/dev/random bs=1 count=1024 > $munge_dir/munge.key
	fi
    fi
    
    # Enable munge
    auto-enable-service munged $0 > $LOG_DIR/munged_enable 2>&1 || true
    service munged restart

    slurm_conf="$LOCALBASE/etc/slurm.conf"
    if [ $NODE_TYPE = 'head' ]; then
	line
	cat << EOM

If you have a slurm.conf file prepared, restore it to $LOCALBASE/etc now.

After you press return, slurm.conf will be opened in $EDITOR.

You can also generate a SLURM config file using the web wizard at:

    $LOCALBASE/share/doc/slurm-*/html/configurator.html

The slurmctld daemon will be started after you exit the editor.

EOM
	line
	pause
	if [ ! -e $slurm_conf ]; then
	    cp $slurm_conf.example $slurm_conf
	fi
	$EDITOR $slurm_conf
	stop_service slurmctld
	auto-enable-service slurmctld $0 > $LOG_DIR/slurmctld_enable 2>&1
	service slurmctld restart
    else
	stop_service slurmd
	auto-enable-service slurmd $0 > $LOG_DIR/slurmd_enable 2>&1
	service slurmd restart
    fi
}


scheduler_config()
{
    line
    if auto-package-installed sysutils/slurm-wlm; then
	slurm_config
    fi
    printf "Done with slurm_config.\n"
    
    if auto-package-installed sysutils/condor; then
	if [ ! -e $LOCALBASE/etc/condor_config ]; then
	    cp $LOCALBASE/etc/condor/condor_config $LOCALBASE/etc
	fi
	if [ ! -e $LOCALBASE/etc/condor_config.local ]; then
	    cp $LOCALBASE/etc/condor/condor_config.local $LOCALBASE/etc
	fi
	cat << EOM

If you have condor_config and condor_config.local files prepared, restore
them to $LOCALBASE/etc now.

EOM
	pause
	$EDITOR $LOCALBASE/etc/condor_config*
    fi
}


nfs_config()
{
    cat << EOM

Some schedulers require a shared spool or config directory.  Check the
documentation for your scheduler before deciding which directories to
share via NFS.

Some other directories you may want to share include:

    /usr/home (or /home)
    /var/cache/pkg

EOM

    line
    nfs_config=`auto-ask nfs-config 'Configure NFS?' y`
    if [ $nfs_config != 'y' ]; then
	return
    fi

    # FIXME: Submit PR
    # NFS may fail to start without this, even if using ZFS
    touch /etc/exports
    auto-append-line 'V4: /' /etc/exports $0
    
    auto-enable-service -s statd rpc_statd $0
    auto-enable-service -s lockd rpc_lockd $0
    
    case $NODE_TYPE in
    head|io)
	# Configure NFS whether or not compute nodes will use it
	# Export /usr (which should include /usr/home)
	# FIXME: Make sure NFS server is enabled in rc.conf first
	# Does not work for multi-homed host
	# ip=`fgrep $head_host $HOSTS | awk ' { print $1 }' | uniq`
	subnet=`auto-ask subnet "\nSubnet for NFS?" 192.168.0.0`

	printf "Updating $RC_CONF...\n"
	# Need UDP for some PXE ROMs
	auto-append-line 'nfs_server_flags="-t -u -n 16"' $RC_CONF $0
	
	# For NFSv4.  Some configs don't need nfsuser.
	# man nfsv4 for details
	auto-append-line 'nfsv4_server_enable="YES"' $RC_CONF $0
	auto-append-line 'nfsuserd_enable="YES"' $RC_CONF $0

	touch /etc/exports  # Must be present for NFS to start
	auto-enable-service nfs_server $0 > $LOG_DIR/nfsd_enable 2>&1
	auto-nfs-restart
	
	share='x'
	while [ 0$share != 0 ]; do
	    line
	    printf "\nMounted filesystems:\n\n"
	    df
	    printf "\nCurrent mounts:\n\n"
	    showmount -e || true
	    printf "\nDirectory to export? [Press enter to quit] "
	    read share
	    
	    if [ 0$share != 0 ]; then
		fs_type=`mount | awk -v share=$share '$3 == share { print $4 }'`
		if [ 0$fs_type = 0'(zfs,' ]; then
		    zfs_share=`mount | awk -v share=$share '$3 == share { print $1 }'`
		    zfs set sharenfs="-maproot=root -network $subnet -mask $NETMASK" $zfs_share
		    cat << EOM

Disabling the ZIL (ZFS Intent Log) will vastly improve NFS write performance
in exchange for increased data loss in the event of power outages and other
sudden failures.  Such loss is generally of no concern in HPC, where the
jos will simply be rerun anyway.

EOM
		    read -p "Disable ZIL? [y]/n " no_zil
		    if [ 0$no_zil != 0y ]; then
			zfs set sync=disabled $zfs_share
		    fi
		else
		    printf "Updating /etc/exports...\n"
		    auto-append-line "$share\t-maproot=0\t-network $subnet -mask $NETMASK\n" /etc/exports $0
		fi
		auto-nfs-restart
	    fi
	done
	zfs get sharenfs
	
	auto-nfs-restart
	printf "\nYou may need to reboot before NFS clients can mount the new shared folder.\n\n"
	pause
	;;
	
    *)
	;;
    esac
    
    pkg clean -ay   # Clean /var/cache/pkg before NFS mounting

    # FIXME: Automatically eliminate local mounts that conflict
    case $NODE_TYPE in
    compute|io|vis)
	fstab=$prefix/etc/spcm/fstab.$NODE_TYPE
	if [ -e $fstab ]; then
	    printf "Automatically adding shares from $fstab...\n"
	    auto-amend-fstab $fstab
	else
	    host='x'
	    while [ 0$host != 0 ]; do
		df
		printf "\nHost name of NFS server? [Press enter to quit] "
		read host
		
		if [ 0$host != 0 ]; then
		    printf "Directory to mount from $host? "
		    read share
		    
		    printf "Local mount point? [$share] "
		    read mount
		    if [ 0$mount = 0 ]; then
			mount=$share
		    fi
		    
		    # FIXME: Make sure mount dir is empty
		    
		    # Unmount local if necessary
		    # First try ZFS
		    zfs_ds=`zfs list | awk -v share=$share '$5 == share { print $1 }'`
		    if [ 0$zfs_ds != 0 ]; then
			zfs set mountpoint=none $zfs_ds
		    fi
		    # Not a ZFS mount?  Maybe some other FS.
		    if mount | fgrep $share; then
			umount $share
		    fi
		    
		    # Configure shared directory
		    found=`awk '$1 == "'${head_host}:$share'" { print $1 }' /etc/fstab`
		    if [ 0$found = 0 ]; then
			printf "\nUpdating /etc/fstab...\n\n"
			printf "# Generated by $0.\n" >> /etc/fstab
			printf "${head_host}:$share\t\t$mount\tnfs\trw,intr,nfsv4\t0\t0\n" >> /etc/fstab
		    else
			printf "Remote filesystem $head_host:$share already configured.\n"
		    fi
		    mkdir -p $share
		fi
	    done
	fi
	mount -a
	;;
	
    *)
	;;
    esac
}


ganglia_config()
{
    ##########################################################################
    #   Head node: bind in both send and receive?
    #   Compute nodes: host = head IP
    ##########################################################################
    
    if [ $NODE_TYPE = head ]; then
	ganglia-web-setup
    fi
    
    line
    if ! auto-package-installed sysutils/ganglia-monitor-core; then
	install_packages sysutils/ganglia-monitor-core
    fi
    
    if grep -q 'name = "unspecified"' $LOCALBASE/etc/gmond.conf; then
	if [ $NODE_TYPE = head ]; then
	    name=$(hostname -s)
	else
	    name=$head_host
	fi
	sed -i '.orig' \
	    -e "s|name = \"unspecified\"|name = \"$name\"|g" \
	    $LOCALBASE/etc/gmond.conf
    fi
    
    # Failing as of 2019-05-18: stop_service gmond
    auto-enable-service gmond cluster-setup \
	> $LOG_DIR/gmond_enable 2>&1
    service gmond restart
    
    if [ $NODE_TYPE = head ]; then
	stop_service gmetad
	auto-enable-service gmetad cluster-setup \
	    > $LOG_DIR/gmetad_enable 2>&1
	service gmetad restart
    fi
}


##########################################################################
#   Function description:
#       Add this host to authorized_hosts on remote host
#
#   Arguments:
#       1) remote host
#       
#   History:
#   Date        Name        Modification
#   2013-03-01  Charlie &   Begin
##########################################################################

ssh_authorize()
{
    if [ $# -lt 1 ] ; then
       echo "usage: ssh-authorize remotehost"
       exit 1
    fi
    remotehost=$1
    
    cd
    
    user=`whoami`
    echo "Authorizing ${user}@${HOST} on $remotehost."
    ssh $remotehost 'umask 077; mkdir -p .ssh'

    # See if key already exists on remote host
    if ssh $remotehost "grep -q ${user}@${HOST} .ssh/authorized_keys"; then
	printf "Key already exists for this host.\n"
	return
    fi
    
    #
    # If a key hasn't been generated then do it.
    #
    if [ ! -f .ssh/id_rsa.pub ] ; then
	printf "Error: ssh_authorize must be called AFTER id_rsa is installed.\n"
	exit 1
    fi
    
    key=`cat .ssh/id_rsa.pub`
    ssh $remotehost "echo $key >> .ssh/authorized_keys ; chmod 600 .ssh/authorized_keys"
}


##########################################################################
#   Function description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2012-07-30  Charlie &   Begin
##########################################################################

ssh_config()
{
    # Verify that nfs_config has been run

    line
    printf "Configuring ssh...\n"
    
    # Permit root login over ssh
    if ! grep -iq '^PermitRootLogin without-password' /etc/ssh/sshd_config; then
	sed -i ".bak" 's|#PermitRootLogin no|PermitRootLogin without-password|g' /etc/ssh/sshd_config
	# killall -HUP sshd
	service sshd reload
	line
	cat << EOM

WARNING: 'PermitRootLogin without-password' has been enabled.

This is necessary for clusters that use keyed SSH between nodes for system
activities.  If you are concerned about security, you can manually
reconfigure the head node.

EOM
    fi
    
    # All nodes should already accept passwordless login from the head node
    # This is set up by auto-pxe-installer-setup and left to the user
    # if they are not using it.
    #if [ $NODE_TYPE = 'head' ]; then
    #    # Disable prompting for new hosts, etc.
    #    cluster-update-all-ssh_config
    #fi
}


##########################################################################
#   Function description:
#       Must be done after NFS config due to use of $HEAD_USR.
#
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2012-07-30  Charlie &   Begin
##########################################################################

resource_limits()
{
    # Set resource limits
    line
    printf "Configuring resource limits...\n"
    if [ ! -e /etc/login.conf.orig ]; then
	cp /etc/login.conf /etc/login.conf.orig
    fi

    cat << EOM
Setting desired resource limits in /etc/login.conf.

It is a good idea to limit vmemoryuse and maxproc on the head node
to prevent it from being overloaded by careless users.

The scheduler should manage limits on compute nodes, but some generous hard
limits on vmemoryuse and maxproc in login.conf can help protect nodes from
fork() bombs and memory leaks that the scheduler does not catch (fast enough).
Usually, something like 10 + the number of cores is a good limit for compute
nodes.

EOM
    # Be generous, but prevent fork bombs
    default_maxproc=$(( `sysctl -n kern.smp.cpus` * 4 ))
    # default_maxproc=32
    maxproc=`auto-ask max-proc "Max processes for the $NODE_TYPE node?" $default_maxproc`
    
    default_vmem=$(( `sysctl -n hw.realmem` / 1024 / 1024 ))m
    vmem=`auto-ask vmem "Max memory use for the $NODE_TYPE node? (use 'm' or 'g' suffix)" $default_vmem`
    
    umask=`auto-ask umask "umask for the $NODE_TYPE node?" 027`
    sed -i '' -e "s/maxproc=unlimited/maxproc=$maxproc/g" \
	     -e "s/vmemoryuse=unlimited/vmemoryuse=$vmem/g" \
	     /etc/login.conf
    cap_mkdb /etc/login.conf
    auto-set-umask $umask
}


##########################################################################
#   Function description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2013-02-15  Charlie &   Begin
##########################################################################

update_system()
{
    line
    printf "Updating system clock...\n"
    stop_service ntpd
    ntpdate -u pool.ntp.org
    auto-enable-service ntpd cluster-setup > /dev/null 2>&1
    service ntpd restart
}


##########################################################################
#   Function description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2013-12-02  Jason Bacon Begin
##########################################################################

install_common_tools()
{
    # Quick-install some common tools
    line
    install_packages devel/gmake net/rsync converters/libiconv \
	editors/nano ftp/wget devel/subversion \
	shells/bash shells/dash
}


##########################################################################
#   Function description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2013-12-02  Jason Bacon Begin
##########################################################################

get_port_install_method()
{
    line
    #resp=`auto-ask use-source "Build ports from source?" n`
    resp=n
    if [ 0$resp = 0'y' ]; then
	export AUTO_BUILD_FROM_SOURCE='yes'
    else
	export AUTO_BUILD_FROM_SOURCE='fall-back'
	if [ x$AUTO_PACKAGEROOT = x ]; then
	    printf "Finding fastest mirror... "
	    export AUTO_PACKAGEROOT=`auto-fastest-mirror`
	    printf "$AUTO_PACKAGEROOT\n"
	fi
    fi
}


##########################################################################
#   Function description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2013-12-02  Jason Bacon Begin
##########################################################################

startup_scripts()
{
    # Install extra login logic
    line
    printf "Installing startup scripts...\n"
    if [ ! -e $LOCALBASE/etc/spcm/profile ]; then
	cp $DATADIR/profile $LOCALBASE/etc/spcm
    fi
    if [ ! -e $LOCALBASE/etc/spcm/csh.login ]; then
	cp $DATADIR/csh.login $LOCALBASE/etc/spcm
    fi
    auto-append-line cluster/profile ". $LOCALBASE/etc/spcm/profile" /etc/profile cluster-setup
    auto-append-line cluster/csh.login "source $LOCALBASE/etc/spcm/csh.login" /etc/csh.login cluster-setup
}


##########################################################################
#   Function description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2013-12-02  Jason Bacon Begin
##########################################################################

enable_procfs()
{
    line
    auto-enable-procfs
}


##########################################################################
#   Function description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2013-12-02  Jason Bacon Begin
##########################################################################

reboot_msg()
{
    line
    cat << EOM

You must reboot to test the new configuration.  If this is the first time
you completed the cluster-setup $NODE_TYPE process, you should reboot now.

EOM
}


##########################################################################
#   Function description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2015-01-30  Charlie &   Begin
##########################################################################

get_head_host()
{
    case $NODE_TYPE in
    compute|io|vis)
	# If cluster-setup was previously completed on this node,
	# get head node from configuration, otherwise ask.
	head_host=''
	while [ 0$head_host = 0 ]; do
	    head_host=`auto-ask head-hostname "\nShort hostname of head node?" login`
	done
	;;
    
    head)
	head_host=`hostname -s`
	hostname -s > $LOCALBASE/etc/spcm/head_node
	;;
    
    esac
}


##########################################################################
#   Main
##########################################################################

if [ $# != 1 ]; then
    usage $0
fi

# Caution: 
# The line below is modified by install. Be careful not to replace
# %%PREFIX%% with a hard-coded prefix from an installed script.
prefix=%%PREFIX%%

NODE_TYPE=$1
case $NODE_TYPE in
head|compute|io|vis)
    ;;
*)
    usage $0
    ;;
esac

if [ `whoami` != root ]; then
    printf "$0 must be run as root.\n"
    exit 1
fi

if ! fgrep -q $(hostname) /etc/hosts; then
    printf "Error: /etc/hosts must contain an entry for this node.\n"
    exit 1
fi

for file in /root/ssh /etc/fstab; do
    if [ -e $file ] && [ ! -e $file.backup ]; then
	printf "Backing up $file...\n"
	cp -R $file $file.backup
    fi
done

##########################################################################
#   Set common variables
##########################################################################

# Recommended layout for dedicated hardware:
# Use netmask of 255.255.128.0.
MAX_NODES=60000

# Misc
LOCALBASE=/usr/local
RC_CONF=/etc/rc.conf
HOSTS=/etc/hosts
HOSTS_ALLOW=/etc/hosts.allow
START_DIR=`pwd`
APACHE_PKG=apache24

# Used by child scripts, so export it
LOG_DIR=$START_DIR/cluster-setup-log
line
printf "Command output will be stored in $LOG_DIR.\n"
line

# Source for default files
DATADIR=$LOCALBASE/share/spcm

# Needed by openjdk, bash, etc.
auto-add-fdesc-mount

auto-ipmi-install || true

# FIXME: Install bsdstats?

# Config files used by spcm
if [ $NODE_TYPE = head ]; then
    CLUSTER_DATA="$LOCALBASE/etc/spcm"
    mkdir -p $CLUSTER_DATA

    if [ -z $EDITOR ]; then
	EDITOR=vi
	export EDITOR
    fi
    
    printf "\nIf you have an old $HOSTS_ALLOW to restore, do it now.\n\n"
    pause
    $EDITOR $HOSTS_ALLOW

    latest=`auto-ask latest-packages 'Use latest packages instead of quarterly?' y`
    if [ $latest != n ]; then
	auto-pkg-latest --defaults
    fi
    
    cat << EOM

It's a good idea to start with a fully updated system, to ensure ports and
package compatibility and maximize security.

EOM
    update=`auto-ask update-system 'Update system before continuing?' y`
    if [ $update != n ]; then
	auto-update-system  # Run interactively
    fi
fi

NETMASK=`auto-ask netmask 'Netmask for local network?' 255.255.128.0`
export NETMASK  # Pass to other scripts like ganglia-web-setup

##########################################################################
#   Begin setup
##########################################################################

# Done by cluster-bootstrap
# auto-pkg-latest

# The following configuration must be done on all nodes, although
# different node types may be configured differently.
update_system               # Start with and NTP update
enable_procfs               # Needed for?
get_port_install_method     # Build ports from source?
install_common_tools        # gmake, etc.
get_head_host
if [ $NODE_TYPE = head ] || [ $NODE_TYPE = compute ]; then
    install_scheduler           # Select and install a scheduler
fi
nfs_config                  # Set up head node NFS server.
ssh_config                  # Enable keys between nodes. After nfs_config.
if [ $NODE_TYPE = head ] || [ $NODE_TYPE = compute ]; then
    scheduler_config            # Configure scheduler.  After ssh_config.
fi
resource_limits             # Configure login.conf.  After nfs_config.
ganglia_config              # Cluster monitoring.

# Configuration steps that only occur on certain node types
case $NODE_TYPE in
head)
    startup_scripts
    ;;

compute|vis)
    install_packages lang/gcc devel/llvm lang/python
    # Install after scheduler.  OpenMPI port has options for scheduler
    # integration.
    install_mpi
    
    # Limit ZFS memory use to keep RAM available for jobs
    compute-node-set-arc-max    
    ;;
*)
    ;;
esac

if [ $NODE_TYPE = head ]; then
    printf "Checking/setting UID limits...\n"
    cluster-lowest-uid
    cluster-highest-uid
fi

# Required by cluster-ssh-keygen, called from cluster-adduser
# FIXME: Remove need for sudo.  Maybe use su -m?
install_packages security/sudo

line
image=$(auto-install-media-distname disc1)
pxedir=/pxeserver/imags/${image%.iso}
if [ $NODE_TYPE = head ] && [ ! -e $pxedir ]; then
    read -p 'Configure PXE installer for other nodes? [y]/n ' pxe
    if [ 0$pxe != 0n ]; then
	save_cwd=$(pwd)
	mkdir -p /root/PXE
	cd /root/PXE
	if [ ! -e $image ]; then
	    auto-fetch-install-media disc1
	    unxz -f $image.xz
	fi
	auto-pxe-installer-setup $image
	cd $save_cwd
    fi
else
    printf "$pxedir already exists.\n"
fi

# Run local site hooks
if [ -e $CONF_DIR/site-mods.sh ]; then
    if auto-file-secure $CONF_DIR/site-mods.sh; then
	$CONF_DIR/site-mods.sh
    else
	printf "ALERT: $CONF_DIR/site-mods.sh is not secure!\n"
	printf "Investigate this problem before proceeding!\n"
	exit 1           
    fi
fi

touch $CONF_DIR/node-configured

# Reboot to test new config after initial setup
reboot_msg
